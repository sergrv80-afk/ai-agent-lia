#!/usr/bin/env python3
"""
GEO-–≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–≥–µ–Ω—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è AI-–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- geo_optimization_agent.py (–ø—Ä–∞–≤–∏–ª–∞ + –∞–Ω–∞–ª–∏–∑)
- geo_llm_agent.py (GPT-5 –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
–¢–µ–º–∞—Ç–∏–∫–∞: AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã, —á–∞—Ç-–±–æ—Ç—ã, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–∞–∂
"""

import os
import re
import json
import requests
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import openai
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

class GEOHybridAgent:
    def __init__(self):
        self.project_root = Path(__file__).parent
        # –ß–∏—Ç–∞–µ–º API –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("‚ùå OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª .env")
        
        self.client = openai.OpenAI(api_key=self.api_key)
        self.MODEL = "gpt-5"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPT-5 –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        
        # SEO —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        self.required_seo_elements = {
            "meta": ["title", "description", "keywords", "author", "robots", "canonical"],
            "opengraph": ["og:title", "og:description", "og:type", "og:url", "og:image", "og:locale", "og:site_name"],
            "twitter": ["twitter:card", "twitter:site", "twitter:creator", "twitter:title", "twitter:description", "twitter:image"],
            "json_ld": ["Article", "WebSite", "BreadcrumbList"]
        }
        
        # LLM-friendly —ç–ª–µ–º–µ–Ω—Ç—ã
        self.llm_optimization_elements = [
            "FAQ –±–ª–æ–∫–∏", "HowTo –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏",
            "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏", "–ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –≤—ã–≤–æ–¥—ã", "–õ–æ–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"
        ]

    def run_hybrid_optimization(self, article_path: str) -> Dict:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é: –∞–Ω–∞–ª–∏–∑ + GPT-5 –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ + –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ"""
        try:
            print(f"üöÄ –ó–∞–ø—É—Å–∫–∞—é –ì–ò–ë–†–ò–î–ù–£–Æ GEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è: {article_path}")
            
            # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å—é —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª–∞ (–±—ã—Å—Ç—Ä–æ)
            print("üìä –≠—Ç–∞–ø 1: –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª–∞...")
            analysis = self.analyze_article(article_path)
            if not analysis["success"]:
                return analysis
            
            # 2. –ü–ª–∞–Ω–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —á–µ—Ä–µ–∑ GPT-5 (—É–º–Ω–æ)
            print("ü§ñ –≠—Ç–∞–ø 2: GPT-5 –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
            llm_plan = self._request_gpt_optimization_plan(article_path, analysis)
            if not llm_plan.get("success"):
                print(f"‚ö†Ô∏è GPT-5 –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å: {llm_plan.get('error')}")
                print("üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º...")
                llm_plan = {"success": False, "data": {}}
            
            # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º GPT-5 –ø–ª–∞–Ω (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if llm_plan.get("success"):
                print("üîß –≠—Ç–∞–ø 3: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ GPT-5 –ø–ª–∞–Ω–∞...")
                gpt_result = self._apply_gpt_plan(article_path, llm_plan["data"])
                if gpt_result.get("success"):
                    print("‚úÖ GPT-5 –ø–ª–∞–Ω –ø—Ä–∏–º–µ–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
                else:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è GPT-5 –ø–ª–∞–Ω–∞: {gpt_result.get('error')}")
            
            # 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º
            print("üîß –≠—Ç–∞–ø 4: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º...")
            optimization_result = self.optimize_article(article_path)
            
            # 5. –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç
            print("üìã –≠—Ç–∞–ø 5: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
            report = self._create_hybrid_report(analysis, llm_plan, optimization_result)
            
            # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
            report_path = self.project_root / f"hybrid_optimization_report_{Path(article_path).stem}.md"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            print(f"‚úÖ –ì–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
            
            return {
                "success": True,
                "article_path": article_path,
                "analysis": analysis,
                "gpt_plan": llm_plan,
                "optimization": optimization_result,
                "report_path": str(report_path),
                "summary": f"–°—Ç–∞—Ç—å—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –≥–∏–±—Ä–∏–¥–Ω—ã–º –º–µ—Ç–æ–¥–æ–º. SEO: {analysis['seo_analysis']['score']}%, LLM: {analysis['llm_analysis']['score']}%"
            }
            
        except Exception as e:
            return {"success": False, "error": f"–û—à–∏–±–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {str(e)}"}

    def _request_gpt_optimization_plan(self, article_path: str, analysis: Dict) -> Dict:
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø–ª–∞–Ω –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É GPT-5"""
        try:
            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏
            article_file = self.project_root / article_path
            with open(article_file, 'r', encoding='utf-8') as f:
                article_content = f.read()
            
            # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è GPT-5
            context = self._collect_context_for_gpt(article_path, analysis, article_content)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è GPT-5
            system_prompt = (
                "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ SEO –∏ LLM-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç–∞—Ç—å—é –∏ —Å–æ–∑–¥–∞–π –ø–ª–∞–Ω –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. "
                "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
            )
            
            user_prompt = self._build_gpt_prompt(context, analysis)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ GPT-5 —á–µ—Ä–µ–∑ Responses API
            response = self.client.responses.create(
                model=self.MODEL,
                input=f"{system_prompt}\n\n{user_prompt}",
                reasoning={"effort": "medium"},   # minimal|low|medium|high
                text={"verbosity": "medium"}     # low|medium|high
            )
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            gpt_response = response.output_text.strip()
            
            print(f"ü§ñ GPT-5 –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω (–¥–ª–∏–Ω–∞: {len(gpt_response)} —Å–∏–º–≤–æ–ª–æ–≤)")
            print(f"üìù –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {gpt_response[:200]}...")
            
            # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–¥–æ–≤—ã–µ –±–ª–æ–∫–∏
            gpt_response = re.sub(r'^```[a-zA-Z]*\n|\n```$', '', gpt_response)
            
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
            json_match = re.search(r'\{.*\}', gpt_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                print(f"üîç –ù–∞–π–¥–µ–Ω JSON (–¥–ª–∏–Ω–∞: {len(json_str)} —Å–∏–º–≤–æ–ª–æ–≤)")
                print(f"üìã JSON: {json_str[:300]}...")
                
                try:
                    # –ü–∞—Ä—Å–∏–º JSON
                    plan = json.loads(json_str)
                    print("‚úÖ JSON —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω")
                    return {"success": True, "data": plan}
                except json.JSONDecodeError as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
                    print(f"üîç –ü—Ä–æ–±–ª–µ–º–Ω—ã–π JSON: {json_str}")
                    return {"success": False, "error": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {str(e)}"}
            else:
                print("‚ùå JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ GPT-5")
                print(f"üîç –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç GPT-5: {gpt_response}")
                return {"success": False, "error": "JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ GPT-5"}
            
        except Exception as e:
            return {"success": False, "error": f"–û—à–∏–±–∫–∞ GPT-5 –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}"}

    def _collect_context_for_gpt(self, article_path: str, analysis: Dict, article_content: str) -> Dict:
        """–°–æ–±–∏—Ä–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è GPT-5"""
        context = {
            "article_path": article_path,
            "article_content": article_content[:15000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
            "seo_analysis": analysis.get("seo_analysis", {}),
            "llm_analysis": analysis.get("llm_analysis", {}),
            "content_analysis": analysis.get("content_analysis", {}),
            "image_analysis": analysis.get("image_analysis", {}),
            "recommendations": analysis.get("recommendations", [])
        }
        return context

    def _build_gpt_prompt(self, context: Dict, analysis: Dict) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è GPT-5"""
        return f"""
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ SEO –∏ LLM-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç–∞—Ç—å—é –∏ —Å–æ–∑–¥–∞–π –ø–ª–∞–Ω –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.

–í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

–¢–ï–ö–£–©–ò–ô –ê–ù–ê–õ–ò–ó:
- SEO Score: {analysis.get('seo_analysis', {}).get('score', 0)}%
- LLM Score: {analysis.get('llm_analysis', {}).get('score', 0)}%
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞ Score: {analysis.get('content_analysis', {}).get('score', 0)}%
- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è Score: {analysis.get('image_analysis', {}).get('score', 0)}%

–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò: {', '.join(analysis.get('recommendations', [])[:5])}

–°–û–ó–î–ê–ô –ü–õ–ê–ù –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –í JSON –§–û–†–ú–ê–¢–ï:

{{
  "meta_improvements": {{
    "title": "—É–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏",
    "description": "—É–ª—É—á—à–µ–Ω–Ω–æ–µ meta –æ–ø–∏—Å–∞–Ω–∏–µ",
    "keywords": "–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é"
  }},
  "content_improvements": {{
    "faq_questions": [
      {{
        "question": "–í–æ–ø—Ä–æ—Å 1",
        "answer": "–û—Ç–≤–µ—Ç 1"
      }},
      {{
        "question": "–í–æ–ø—Ä–æ—Å 2", 
        "answer": "–û—Ç–≤–µ—Ç 2"
      }}
    ],
    "summary_text": "–∫—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥ —Ä–∞–∑–¥–µ–ª–∞",
    "internal_links": [
      {{
        "text": "—Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏",
        "url": "/#section"
      }}
    ]
  }},
  "technical_improvements": {{
    "alt_tags": [
      {{
        "src_pattern": "—á–∞—Å—Ç—å src",
        "alt": "–æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
      }}
    ],
    "json_ld_schemas": ["Article", "FAQPage"]
  }},
  "priority": "high",
  "estimated_impact": "–æ–ø–∏—Å–∞–Ω–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞"
}}

–ü–†–ê–í–ò–õ–ê:
1. JSON –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤–∞–ª–∏–¥–Ω—ã–º
2. –í—Å–µ —Å–∫–æ–±–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–∫—Ä—ã—Ç—ã
3. –ò—Å–ø–æ–ª—å–∑—É–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
4. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º
5. –ù–µ –¥–æ–±–∞–≤–ª—è–π —Ç–µ–∫—Å—Ç –≤–Ω–µ JSON
"""

    def _apply_gpt_plan(self, article_path: str, plan: Dict) -> Dict:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø–ª–∞–Ω –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ—Ç GPT-5"""
        try:
            article_file = self.project_root / article_path
            
            # –°–æ–∑–¥–∞–µ–º backup
            backup_path = article_file.with_suffix('.gpt.backup.html')
            with open(article_file, 'r', encoding='utf-8') as f:
                original_content = f.read()
            
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(original_content)
            
            optimized_content = original_content
            applied_changes = []
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–∏—è meta —Ç–µ–≥–æ–≤
            if "meta_improvements" in plan:
                meta_changes = self._apply_meta_improvements(optimized_content, plan["meta_improvements"])
                if meta_changes["changes"] > 0:
                    optimized_content = meta_changes["content"]
                    applied_changes.append(f"Meta —Ç–µ–≥–∏: {meta_changes['changes']} –∏–∑–º–µ–Ω–µ–Ω–∏–π")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if "content_improvements" in plan:
                content_changes = self._apply_content_improvements(optimized_content, plan["content_improvements"])
                if content_changes["changes"] > 0:
                    optimized_content = content_changes["content"]
                    applied_changes.append(f"–ö–æ–Ω—Ç–µ–Ω—Ç: {content_changes['changes']} —É–ª—É—á—à–µ–Ω–∏–π")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è
            if "technical_improvements" in plan:
                tech_changes = self._apply_technical_improvements(optimized_content, plan["technical_improvements"])
                if tech_changes["changes"] > 0:
                    optimized_content = tech_changes["content"]
                    applied_changes.append(f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ: {tech_changes['changes']} —É–ª—É—á—à–µ–Ω–∏–π")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é
            with open(article_file, 'w', encoding='utf-8') as f:
                f.write(optimized_content)
            
            return {
                "success": True,
                "backup_path": str(backup_path),
                "changes_applied": applied_changes,
                "total_changes": len(applied_changes)
            }
            
        except Exception as e:
            return {"success": False, "error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è GPT-5 –ø–ª–∞–Ω–∞: {str(e)}"}

    def _apply_meta_improvements(self, content: str, meta_plan: Dict) -> Dict:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è meta —Ç–µ–≥–æ–≤"""
        changes = 0
        
        # –û–±–Ω–æ–≤–ª—è–µ–º title
        if "title" in meta_plan and meta_plan["title"]:
            title_pattern = r'<title>(.*?)</title>'
            if re.search(title_pattern, content):
                content = re.sub(title_pattern, f'<title>{meta_plan["title"]}</title>', content)
                changes += 1
        
        # –û–±–Ω–æ–≤–ª—è–µ–º description
        if "description" in meta_plan and meta_plan["description"]:
            desc_pattern = r'<meta name="description" content="[^"]*"'
            if re.search(desc_pattern, content):
                content = re.sub(desc_pattern, f'<meta name="description" content="{meta_plan["description"]}"', content)
                changes += 1
        
        return {"content": content, "changes": changes}

    def _apply_content_improvements(self, content: str, content_plan: Dict) -> Dict:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        changes = 0
        
        # –î–æ–±–∞–≤–ª—è–µ–º FAQ –±–ª–æ–∫–∏
        if "faq_questions" in content_plan and content_plan["faq_questions"]:
            faq_html = self._generate_faq_html(content_plan["faq_questions"])
            if faq_html:
                content = self._insert_faq_section(content, faq_html)
                changes += 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–π –≤—ã–≤–æ–¥
        if "summary_text" in content_plan and content_plan["summary_text"]:
            summary_html = self._generate_summary_html(content_plan["summary_text"])
            if summary_html:
                content = self._insert_summary_section(content, summary_html)
                changes += 1
        
        return {"content": content, "changes": changes}

    def _apply_technical_improvements(self, content: str, tech_plan: Dict) -> Dict:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è"""
        changes = 0
        
        # –û–±–Ω–æ–≤–ª—è–µ–º alt —Ç–µ–≥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if "alt_tags" in tech_plan and tech_plan["alt_tags"]:
            for alt_item in tech_plan["alt_tags"]:
                if "src_pattern" in alt_item and "alt" in alt_item:
                    content = self._update_image_alt(content, alt_item["src_pattern"], alt_item["alt"])
                    changes += 1
        
        return {"content": content, "changes": changes}

    def _generate_faq_html(self, faq_items: List[Dict]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTML –¥–ª—è FAQ –±–ª–æ–∫–∞"""
        if not faq_items:
            return ""
        
        faq_html = [
            '<section id="faq" class="alt">',
            '  <div class="container">',
            '    <h2 class="h2">–ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã</h2>',
            '    <div class="faq-grid">'
        ]
        
        for item in faq_items[:6]:  # –ú–∞–∫—Å–∏–º—É–º 6 –≤–æ–ø—Ä–æ—Å–æ–≤
            question = item.get("question", "–í–æ–ø—Ä–æ—Å")
            answer = item.get("answer", "–û—Ç–≤–µ—Ç")
            faq_html.extend([
                '      <article class="faq-item">',
                f'        <h3>{question}</h3>',
                f'        <p>{answer}</p>',
                '      </article>'
            ])
        
        faq_html.extend(['    </div>', '  </div>', '</section>'])
        return '\n'.join(faq_html)

    def _generate_summary_html(self, summary_text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTML –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞"""
        return f'''
<!-- ============== –ö–û–ù–¢–ï–ù–¢–ù–´–ô –í–´–í–û–î ============== -->
<section class="content-summary-section">
  <div class="container">
    <div class="content-summary" style="background: var(--bg-alt); padding: 32px; border-radius: 20px; margin: 48px 0; text-align: center;">
      <h2 class="h2">üìã –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥</h2>
      <p style="font-size: 18px; line-height: 1.6; margin: 24px 0; color: var(--text);">
        {summary_text}
      </p>
      <div style="display: flex; gap: 16px; justify-content: center; flex-wrap: wrap; margin-top: 24px;">
        <a href="/#signup" class="btn btn-primary">–ü–æ–ª—É—á–∏—Ç—å CRM –±–µ—Å–ø–ª–∞—Ç–Ω–æ</a>
        <a href="/#ai-team" class="btn btn-ghost">–£–∑–Ω–∞—Ç—å –ø—Ä–æ AI-–∫–æ–º–∞–Ω–¥—É</a>
      </div>
    </div>
  </div>
</section>'''

    def _insert_faq_section(self, content: str, faq_html: str) -> str:
        """–í—Å—Ç–∞–≤–ª—è–µ—Ç FAQ —Å–µ–∫—Ü–∏—é –ø–µ—Ä–µ–¥ —Ñ—É—Ç–µ—Ä–æ–º"""
        footer_marker = r'<!-- ============== FOOTER ============== -->'
        if re.search(footer_marker, content):
            return re.sub(footer_marker, f'{faq_html}\n\n{footer_marker}', content, count=1)
        return content

    def _insert_summary_section(self, content: str, summary_html: str) -> str:
        """–í—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–π –≤—ã–≤–æ–¥ –ø–µ—Ä–µ–¥ FAQ"""
        faq_marker = r'<!-- ============== FAQ SECTION ============== -->'
        if re.search(faq_marker, content):
            return re.sub(faq_marker, f'{summary_html}\n\n{faq_marker}', content, count=1)
        return content

    def _update_image_alt(self, content: str, src_pattern: str, alt_text: str) -> str:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç alt —Ç–µ–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        img_pattern = rf'<img([^>]*?)src="[^"]*{re.escape(src_pattern)}[^"]*"([^>]*?)>'
        
        def replace_img(match):
            img_attrs = match.group(1) + match.group(2)
            if 'alt=' in img_attrs:
                return re.sub(r'alt="[^"]*"', f'alt="{alt_text}"', match.group(0))
            else:
                return match.group(0)[:-1] + f' alt="{alt_text}">'
        
        return re.sub(img_pattern, replace_img, content)

    def analyze_article(self, article_path: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é –∏ –≤—ã—è–≤–ª—è–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ SEO —ç–ª–µ–º–µ–Ω—Ç—ã"""
        try:
            article_file = self.project_root / article_path
            if not article_file.exists():
                return {"success": False, "error": f"–§–∞–π–ª {article_path} –Ω–µ –Ω–∞–π–¥–µ–Ω"}
            
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç–∞—Ç—å—é: {article_path}")
            
            # –ê–Ω–∞–ª–∏–∑ SEO —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            seo_analysis = self._analyze_seo_elements(content)
            
            # –ê–Ω–∞–ª–∏–∑ LLM-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            llm_analysis = self._analyze_llm_optimization(content)
            
            # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_analysis = self._analyze_content_structure(content)
            
            # –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            image_analysis = self._analyze_images(content)
            
            return {
                "success": True,
                "article_path": article_path,
                "seo_analysis": seo_analysis,
                "llm_analysis": llm_analysis,
                "content_analysis": content_analysis,
                "image_analysis": image_analysis,
                "recommendations": self._generate_recommendations(seo_analysis, llm_analysis, content_analysis, image_analysis)
            }
            
        except Exception as e:
            return {"success": False, "error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"}

    def _analyze_seo_elements(self, content: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ SEO —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        analysis = {
            "meta_tags": {},
            "opengraph": {},
            "twitter": {},
            "json_ld": {},
            "score": 0,
            "missing": []
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ meta —Ç–µ–≥–æ–≤
        for tag in self.required_seo_elements["meta"]:
            if tag == "title":
                pattern = r'<title>(.*?)</title>'
            elif tag == "description":
                pattern = r'<meta name="description" content="(.*?)"'
            elif tag == "keywords":
                pattern = r'<meta name="keywords" content="(.*?)"'
            elif tag == "author":
                pattern = r'<meta name="author" content="(.*?)"'
            elif tag == "robots":
                pattern = r'<meta name="robots" content="(.*?)"'
            elif tag == "canonical":
                pattern = r'<link rel="canonical" href="(.*?)"'
            
            match = re.search(pattern, content)
            if match:
                analysis["meta_tags"][tag] = {"found": True, "value": match.group(1)}
            else:
                analysis["meta_tags"][tag] = {"found": False, "value": None}
                analysis["missing"].append(f"meta:{tag}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ OpenGraph —Ç–µ–≥–æ–≤
        for tag in self.required_seo_elements["opengraph"]:
            pattern = rf'<meta property="{tag}" content="(.*?)"'
            match = re.search(pattern, content)
            if match:
                analysis["opengraph"][tag] = {"found": True, "value": match.group(1)}
            else:
                analysis["opengraph"][tag] = {"found": False, "value": None}
                analysis["missing"].append(f"og:{tag}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Twitter —Ç–µ–≥–æ–≤
        for tag in self.required_seo_elements["twitter"]:
            pattern = rf'<meta name="{tag}" content="(.*?)"'
            match = re.search(pattern, content)
            if match:
                analysis["twitter"][tag] = {"found": True, "value": match.group(1)}
            else:
                analysis["twitter"][tag] = {"found": False, "value": None}
                analysis["missing"].append(f"og:{tag}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ JSON-LD —Å—Ö–µ–º
        json_ld_pattern = r'<script type="application/ld\+json">(.*?)</script>'
        json_ld_blocks = re.findall(json_ld_pattern, content, re.DOTALL)
        
        for block in json_ld_blocks:
            try:
                data = json.loads(block)
                if "@type" in data:
                    schema_type = data["@type"]
                    analysis["json_ld"][schema_type] = {"found": True, "data": data}
            except json.JSONDecodeError:
                continue
        
        # –ü–æ–¥—Å—á–µ—Ç score
        total_elements = len(self.required_seo_elements["meta"]) + len(self.required_seo_elements["opengraph"]) + len(self.required_seo_elements["twitter"])
        found_elements = sum(1 for tag in analysis["meta_tags"].values() if tag["found"])
        found_elements += sum(1 for tag in analysis["opengraph"].values() if tag["found"])
        found_elements += sum(1 for tag in analysis["twitter"].values() if tag["found"])
        
        analysis["score"] = round((found_elements / total_elements) * 100, 1)
        
        return analysis

    def _analyze_llm_optimization(self, content: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç LLM-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        analysis = {
            "faq_blocks": 0,
            "howto_instructions": 0,
            "structured_lists": 0,
            "semantic_headings": 0,
            "content_summaries": 0,
            "local_keywords": 0,
            "score": 0
        }
        
        # –ü–æ–¥—Å—á–µ—Ç FAQ –±–ª–æ–∫–æ–≤
        faq_pattern = r'<(?:article|div)[^>]*class="faq-item"[^>]*>'
        analysis["faq_blocks"] = len(re.findall(faq_pattern, content))
        
        # –ü–æ–¥—Å—á–µ—Ç HowTo –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        howto_pattern = r'<ol>|<ul>|<li>'
        analysis["howto_instructions"] = len(re.findall(howto_pattern, content))
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤
        list_pattern = r'<ul[^>]*>.*?</ul>|<ol[^>]*>.*?</ol>'
        analysis["structured_lists"] = len(re.findall(list_pattern, content, re.DOTALL))
        
        # –ü–æ–¥—Å—á–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        heading_pattern = r'<h[1-6][^>]*>.*?</h[1-6]>'
        analysis["semantic_headings"] = len(re.findall(heading_pattern, content))
        
        # –ü–æ–¥—Å—á–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤
        summary_pattern = r'<p[^>]*>.*?(?:–≤ –∏—Ç–æ–≥–µ|–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ|—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º|–∏—Ç–∞–∫).*?</p>'
        analysis["content_summaries"] = len(re.findall(summary_pattern, content, re.IGNORECASE))
        
        # –ü–æ–¥—Å—á–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        local_keywords = ["—Å–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã", "–∫–ª–∏–Ω–∏–∫–∞", "—Ñ–∏—Ç–Ω–µ—Å", "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "–∞–≤—Ç–æ–º–æ–π–∫–∞", "—Ä–µ–º–æ–Ω—Ç"]
        analysis["local_keywords"] = sum(1 for keyword in local_keywords if keyword.lower() in content.lower())
        
        # –ü–æ–¥—Å—á–µ—Ç score
        max_score = 6
        current_score = sum([
            min(analysis["faq_blocks"], 2),  # –ú–∞–∫—Å–∏–º—É–º 2 –±–∞–ª–ª–∞ –∑–∞ FAQ
            min(analysis["howto_instructions"], 2),  # –ú–∞–∫—Å–∏–º—É–º 2 –±–∞–ª–ª–∞ –∑–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            min(analysis["structured_lists"], 1),  # –ú–∞–∫—Å–∏–º—É–º 1 –±–∞–ª–ª –∑–∞ —Å–ø–∏—Å–∫–∏
            min(analysis["semantic_headings"], 1)  # –ú–∞–∫—Å–∏–º—É–º 1 –±–∞–ª–ª –∑–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        ])
        
        analysis["score"] = round((current_score / max_score) * 100, 1)
        
        return analysis

    def _analyze_content_structure(self, content: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        analysis = {
            "h1_count": 0,
            "h2_count": 0,
            "h3_count": 0,
            "paragraphs": 0,
            "images": 0,
            "links": 0,
            "cta_blocks": 0,
            "score": 0
        }
        
        # –ü–æ–¥—Å—á–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        analysis["h1_count"] = len(re.findall(r'<h1[^>]*>', content))
        analysis["h2_count"] = len(re.findall(r'<h2[^>]*>', content))
        analysis["h3_count"] = len(re.findall(r'<h3[^>]*>', content))
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        analysis["paragraphs"] = len(re.findall(r'<p[^>]*>', content))
        
        # –ü–æ–¥—Å—á–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        analysis["images"] = len(re.findall(r'<img[^>]*>', content))
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Å—ã–ª–æ–∫
        analysis["links"] = len(re.findall(r'<a[^>]*>', content))
        
        # –ü–æ–¥—Å—á–µ—Ç CTA –±–ª–æ–∫–æ–≤
        cta_pattern = r'<a[^>]*class="[^"]*btn[^"]*"[^>]*>'
        analysis["cta_blocks"] = len(re.findall(cta_pattern, content))
        
        # –ü–æ–¥—Å—á–µ—Ç score
        score = 0
        if analysis["h1_count"] == 1:  # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω H1
            score += 20
        if 4 <= analysis["h2_count"] <= 6:  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ H2
            score += 20
        if analysis["paragraphs"] >= 10:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            score += 20
        if analysis["cta_blocks"] >= 4:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ CTA
            score += 20
        if analysis["images"] > 0:  # –ï—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            score += 20
        
        analysis["score"] = score
        
        return analysis

    def _analyze_images(self, content: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é"""
        analysis = {
            "total_images": 0,
            "with_alt": 0,
            "with_title": 0,
            "optimized": 0,
            "score": 0
        }
        
        # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        img_pattern = r'<img[^>]*>'
        images = re.findall(img_pattern, content)
        analysis["total_images"] = len(images)
        
        if analysis["total_images"] == 0:
            analysis["score"] = 100  # –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π - –∏–¥–µ–∞–ª—å–Ω–æ
            return analysis
        
        for img in images:
            has_alt = 'alt=' in img
            has_title = 'title=' in img
            
            if has_alt:
                analysis["with_alt"] += 1
            if has_title:
                analysis["with_title"] += 1
            if has_alt and has_title:
                analysis["optimized"] += 1
        
        # –ü–æ–¥—Å—á–µ—Ç score
        if analysis["total_images"] > 0:
            analysis["score"] = round((analysis["optimized"] / analysis["total_images"]) * 100, 1)
        
        return analysis

    def _generate_recommendations(self, seo_analysis: Dict, llm_analysis: Dict, content_analysis: Dict, image_analysis: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        recommendations = []
        
        # SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if seo_analysis["score"] < 80:
            for missing in seo_analysis["missing"][:5]:  # –ú–∞–∫—Å–∏–º—É–º 5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
                recommendations.append(f"–î–æ–±–∞–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–π SEO —ç–ª–µ–º–µ–Ω—Ç: {missing}")
        
        # LLM —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if llm_analysis["score"] < 70:
            if llm_analysis["faq_blocks"] == 0:
                recommendations.append("–î–æ–±–∞–≤–∏—Ç—å FAQ –±–ª–æ–∫ —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏")
            if llm_analysis["content_summaries"] == 0:
                recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –≤—ã–≤–æ–¥—ã –≤ –∫–æ–Ω—Ü–µ —Ä–∞–∑–¥–µ–ª–æ–≤")
            if llm_analysis["local_keywords"] < 3:
                recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if content_analysis["score"] < 80:
            if content_analysis["h1_count"] != 1:
                recommendations.append("–î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω H1 –∑–∞–≥–æ–ª–æ–≤–æ–∫")
            if content_analysis["h2_count"] < 4:
                recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ H2 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è")
            if content_analysis["cta_blocks"] < 4:
                recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –ø—Ä–∏–∑—ã–≤–æ–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        if image_analysis["score"] < 100:
            recommendations.append("–î–æ–±–∞–≤–∏—Ç—å alt –∏ title —Ç–µ–≥–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        return recommendations[:8]  # –ú–∞–∫—Å–∏–º—É–º 8 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

    def optimize_article(self, article_path: str) -> Dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º"""
        try:
            print(f"üîß –í—ã–ø–æ–ª–Ω—è—é –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º: {article_path}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å—é
            analysis = self.analyze_article(article_path)
            if not analysis["success"]:
                return analysis
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            optimization_result = self._generate_missing_elements(article_path, analysis)
            
            return {
                "success": True,
                "article_path": article_path,
                "analysis": analysis,
                "optimization": optimization_result,
                "elements_generated": optimization_result.get("elements_generated", [])
            }
            
        except Exception as e:
            return {"success": False, "error": f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {str(e)}"}

    def _generate_missing_elements(self, article_path: str, analysis: Dict) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ SEO —ç–ª–µ–º–µ–Ω—Ç—ã"""
        try:
            article_file = self.project_root / article_path
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            optimized_content = content
            generated_elements = []
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ alt —Ç–µ–≥–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if analysis["image_analysis"]["score"] < 100:
                optimized_content, alt_tags_generated = self._generate_alt_tags(optimized_content)
                if alt_tags_generated > 0:
                    generated_elements.append(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {alt_tags_generated} alt —Ç–µ–≥–æ–≤")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ meta —Ç–µ–≥–∏
            if analysis["seo_analysis"]["score"] < 100:
                optimized_content, meta_tags_generated = self._generate_missing_meta_tags(optimized_content, analysis)
                if meta_tags_generated > 0:
                    generated_elements.append(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {meta_tags_generated} meta —Ç–µ–≥–æ–≤")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º LLM-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            if analysis["llm_analysis"]["score"] < 70:
                optimized_content, llm_elements_generated = self._generate_llm_optimized_content(optimized_content, analysis)
                if llm_elements_generated > 0:
                    generated_elements.append(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {llm_elements_generated} LLM-—ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é
            backup_path = article_file.with_suffix('.rules.backup.html')
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            with open(article_file, 'w', encoding='utf-8') as f:
                f.write(optimized_content)
            
            return {
                "elements_generated": generated_elements,
                "backup_created": str(backup_path),
                "content_optimized": True
            }
            
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {str(e)}"}

    def _generate_alt_tags(self, content: str) -> Tuple[str, int]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç alt —Ç–µ–≥–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        img_pattern = r'<img([^>]*?)>'
        generated_count = 0
        
        def replace_img(match):
            nonlocal generated_count
            img_attrs = match.group(1)
            
            # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å alt, –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
            if 'alt=' in img_attrs:
                return match.group(0)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º alt –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            alt_text = "SmartVizitka - –ë–∏–∑–Ω–µ—Å-–∞–≤—Ç–æ–ø–∏–ª–æ—Ç —Å AI"
            new_img = f'<img{img_attrs} alt="{alt_text}">'
            generated_count += 1
            
            return new_img
        
        optimized_content = re.sub(img_pattern, replace_img, content)
        return optimized_content, generated_count

    def _generate_missing_meta_tags(self, content: str, analysis: Dict) -> Tuple[str, int]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ meta —Ç–µ–≥–∏"""
        generated_count = 0
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ meta —Ç–µ–≥–∏
        head_pattern = r'(<head[^>]*>)([\s\S]*?)(</head>)'
        head_match = re.search(head_pattern, content)
        
        if head_match:
            head_open, head_body, head_close = head_match.groups()
            
            # –î–æ–±–∞–≤–ª—è–µ–º keywords –µ—Å–ª–∏ –Ω–µ—Ç
            if not re.search(r'<meta name="keywords"', head_body):
                keywords_meta = '  <meta name="keywords" content="SmartVizitka, CRM –±–µ—Å–ø–ª–∞—Ç–Ω–æ, –±–∏–∑–Ω–µ—Å-–∞–≤—Ç–æ–ø–∏–ª–æ—Ç, AI, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è" />'
                head_body = head_body + '\n' + keywords_meta
                generated_count += 1
            
            # –î–æ–±–∞–≤–ª—è–µ–º author –µ—Å–ª–∏ –Ω–µ—Ç
            if not re.search(r'<meta name="author"', head_body):
                author_meta = '  <meta name="author" content="SmartVizitka" />'
                head_body = head_body + '\n' + author_meta
                generated_count += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º head
            content = head_open + head_body + head_close
        
        return content, generated_count

    def _generate_llm_optimized_content(self, content: str, analysis: Dict) -> Tuple[str, int]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç LLM-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"""
        generated_count = 0
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –≤—ã–≤–æ–¥—ã –≤ –∫–æ–Ω—Ü–µ —Ä–∞–∑–¥–µ–ª–æ–≤
        if analysis["llm_analysis"]["content_summaries"] == 0:
            # –ò—â–µ–º –∫–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –ø–µ—Ä–µ–¥ FAQ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥
            faq_pattern = r'(<!-- ============== FAQ SECTION ============== -->)'
            
            def add_summary(match):
                nonlocal generated_count
                summary_html = '''
                <!-- ============== –ö–û–ù–¢–ï–ù–¢–ù–´–ô –í–´–í–û–î ============== -->
                <section class="content-summary-section">
                  <div class="container">
                    <div class="content-summary" style="background: var(--bg-alt); padding: 32px; border-radius: 20px; margin: 48px 0; text-align: center;">
                      <h2 class="h2">üìã –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥</h2>
                      <p style="font-size: 18px; line-height: 1.6; margin: 24px 0; color: var(--text);">
                        AI-–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–¥–∞–∂ –∏ –ª–∏–¥–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø–æ–∑–≤–æ–ª—è—è –±–∏–∑–Ω–µ—Å—É —Ä–∞–±–æ—Ç–∞—Ç—å 24/7 –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º–∏ –∏ CRM-—Å–∏—Å—Ç–µ–º–∞–º–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç seamless-–≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å—ã.
                      </p>
                      <div style="display: flex; gap: 16px; justify-content: center; flex-wrap: wrap; margin-top: 24px;">
                        <a href="/#trial" class="btn btn-primary">–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –±–æ—Ç–∞</a>
                        <a href="/#contact" class="btn btn-ghost">–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é</a>
                      </div>
                    </div>
                  </div>
                </section>'''
                generated_count += 1
                return summary_html + '\n\n' + match.group(1)
            
            content = re.sub(faq_pattern, add_summary, content, count=1)
        
        return content, generated_count

    def _create_hybrid_report(self, analysis: Dict, gpt_plan: Dict, optimization: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –≥–∏–±—Ä–∏–¥–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        report = f"""
# üöÄ –û—Ç—á–µ—Ç –ø–æ –≥–∏–±—Ä–∏–¥–Ω–æ–π GEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

## üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏
- **SEO Score**: {analysis.get('seo_analysis', {}).get('score', 0)}%
- **LLM Score**: {analysis.get('llm_analysis', {}).get('score', 0)}%
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∞ Score**: {analysis.get('content_analysis', {}).get('score', 0)}%
- **–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è Score**: {analysis.get('image_analysis', {}).get('score', 0)}%

## ü§ñ GPT-5 –ø–ª–∞–Ω –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
"""
        
        if gpt_plan.get("success"):
            plan_data = gpt_plan.get("data", {})
            report += f"""
- **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: {plan_data.get('priority', '–Ω–µ —É–∫–∞–∑–∞–Ω')}
- **–û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç**: {plan_data.get('estimated_impact', '–Ω–µ —É–∫–∞–∑–∞–Ω')}
- **Meta —É–ª—É—á—à–µ–Ω–∏—è**: {'–î–∞' if 'meta_improvements' in plan_data else '–ù–µ—Ç'}
- **–ö–æ–Ω—Ç–µ–Ω—Ç —É–ª—É—á—à–µ–Ω–∏—è**: {'–î–∞' if 'content_improvements' in plan_data else '–ù–µ—Ç'}
- **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è**: {'–î–∞' if 'technical_improvements' in plan_data else '–ù–µ—Ç'}
"""
        else:
            report += "- **GPT-5 –ø–ª–∞–Ω**: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å\n"
        
        report += f"""
## üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è
{chr(10).join(f"- {change}" for change in optimization.get('elements_generated', []))}

## üìà –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
{chr(10).join(f"- {rec}" for rec in analysis.get('recommendations', []))}

---
*–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω –≥–∏–±—Ä–∏–¥–Ω—ã–º GEO-–∞–≥–µ–Ω—Ç–æ–º AI-–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å GPT-5*
"""
        return report

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è AI-–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"""
    import sys
    
    agent = GEOHybridAgent()
    
    if len(sys.argv) < 2:
        print("üöÄ GEO-–≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–≥–µ–Ω—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ AI-–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç")
        print("üéØ –¢–µ–º–∞—Ç–∏–∫–∞: AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã, —á–∞—Ç-–±–æ—Ç—ã, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–∞–∂")
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  python3 geo_hybrid_agent.py <–ø—É—Ç—å_–∫_—Å—Ç–∞—Ç—å–µ>")
        print("  python3 geo_hybrid_agent.py hybrid <–ø—É—Ç—å_–∫_—Å—Ç–∞—Ç—å–µ>")
        return
    
    command = sys.argv[1]
    article_path = sys.argv[2] if len(sys.argv) > 2 else None
    
    if command == "hybrid" and article_path:
        print("üöÄ –ó–∞–ø—É—Å–∫–∞—é –≥–∏–±—Ä–∏–¥–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é...")
        result = agent.run_hybrid_optimization(article_path)
        if result["success"]:
            print(f"‚úÖ {result['summary']}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
    
    elif article_path:
        print("üöÄ –ó–∞–ø—É—Å–∫–∞—é –≥–∏–±—Ä–∏–¥–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é...")
        result = agent.run_hybrid_optimization(article_path)
        if result["success"]:
            print(f"‚úÖ {result['summary']}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
    
    else:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: hybrid <–ø—É—Ç—å_–∫_—Å—Ç–∞—Ç—å–µ> –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ <–ø—É—Ç—å_–∫_—Å—Ç–∞—Ç—å–µ>")

if __name__ == "__main__":
    main()
